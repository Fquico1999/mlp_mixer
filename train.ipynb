{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8082e8c6-ac81-4507-b422-1eb71758647d",
   "metadata": {},
   "source": [
    "# Training Loop for MLP-Mixer\n",
    "We start with downloading the CIFAR10 and CIFAR100 datasets for fine-tuning, as well as the pre-trained weights from google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baa48a41-974d-4c42-a137-1ba99e665949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mixer import MLPMixer\n",
    "from utils import get_data\n",
    "from utils import WarmupLinearSchedule, WarmupCosineSchedule\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7bdafdb-50c5-4063-a4b0-eeecd92357f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "#Download CIFAR datasets\n",
    "cifar10 = CIFAR10(data_dir,train=True , download=True)\n",
    "cifar100 = CIFAR100(data_dir,train=True , download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d918a6-4e3c-4692-b138-86edea4abb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mixer-B_16.npz', <http.client.HTTPMessage at 0x23b2cb039a0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download base model pre-trained on imagenet\n",
    "import urllib.request\n",
    "url = 'https://storage.googleapis.com/mixer_models/imagenet21k/Mixer-B_16.npz'\n",
    "filename = 'Mixer-B_16.npz'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90ad6881-dd79-4c1d-a168-3affaa56c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_dir, train_batch_size,eval_batch_size, gradient_accumulation_steps, \n",
    "          max_grad_norm, learning_rate, warmup_steps, num_steps, \n",
    "          weight_decay, decay_type,  device, eval_every):\n",
    "    \n",
    "    \"\"\" Train the model \"\"\"\n",
    "    train_batch_size = train_batch_size // gradient_accumulation_steps\n",
    "\n",
    "    # Get train and test data\n",
    "    train_loader, test_loader = get_data(data_dir, img_size, train_batch_size, eval_batch_size)\n",
    "\n",
    "    # Prepare optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=weight_decay)\n",
    "    t_total = num_steps\n",
    "    if decay_type == \"cosine\":\n",
    "        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "    else:\n",
    "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    model.zero_grad()\n",
    "    losses = AverageMeter()\n",
    "    global_step, best_acc = 0, 0\n",
    "    while True:\n",
    "        model.train()\n",
    "        epoch_iterator = tqdm(train_loader,\n",
    "                              desc=\"Training (X / X Steps) (loss=X.X)\",\n",
    "                              bar_format=\"{l_bar}{r_bar}\",\n",
    "                              dynamic_ncols=True)\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            x, y = batch\n",
    "            loss = model(x, y)\n",
    "\n",
    "            if  gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (step + 1) %  gradient_accumulation_steps == 0:\n",
    "                losses.update(loss.item()*gradient_accumulation_steps)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),max_grad_norm)\n",
    "                scheduler.step()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                epoch_iterator.set_description(\n",
    "                    \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, t_total, losses.val)\n",
    "                )\n",
    "                if global_step % eval_every == 0:\n",
    "                    accuracy = valid(model, test_loader, global_step, device)\n",
    "                    if best_acc < accuracy:\n",
    "                        save_model(model, output_dir='.', name='Mixer-B_16')\n",
    "                        best_acc = accuracy\n",
    "                    model.train()\n",
    "\n",
    "                if global_step % t_total == 0:\n",
    "                    break\n",
    "        losses.reset()\n",
    "        if global_step % t_total == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e283477-b345-4ab6-b28f-8e450938e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def save_model(model, output_dir, name):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(output_dir, \"%s_checkpoint.bin\" % name)\n",
    "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
    "\n",
    "\n",
    "def setup(args):\n",
    "    # Prepare model\n",
    "    config = CONFIGS[args.model_type]\n",
    "\n",
    "    num_classes = 10\n",
    "\n",
    "    model = MLPMixer(config, args.img_size, num_classes=num_classes, patch_size=16, zero_head=True)\n",
    "    model.load_from(np.load(args.pretrained_dir))\n",
    "    model.to(args.device)\n",
    "    num_params = count_parameters(model)\n",
    "\n",
    "    logger.info(\"{}\".format(config))\n",
    "    logger.info(\"Training parameters %s\", args)\n",
    "    logger.info(\"Total Parameter: \\t%2.1fM\" % num_params)\n",
    "    return args, model\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params/1000000\n",
    "\n",
    "\n",
    "\n",
    "def valid(model, test_loader, global_step, device):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0]\n",
    "            eval_loss = loss_fct(logits.view(-1, model.n_classes), y.view(-1))\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f56ec6c0-fc45-4bfa-b62e-1107a97f15fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 0.436122 M\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (50 / 10000 Steps) (loss=1.97738):   0%|| 49/50000 [00:15<1:42:53,  8.09it/s]\n",
      "Validating... (loss=X.X):   0%|| 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "Validating... (loss=2.78550):   0%|| 0/10000 [00:06<?, ?it/s]\u001b[A\n",
      "Validating... (loss=2.78550):   0%|| 1/10000 [00:06<19:10:28,  6.90s/it]\u001b[A\n",
      "Training (50 / 10000 Steps) (loss=1.97738):   0%|| 49/50000 [00:22<6:20:56,  2.19it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m eval_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdecay_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecay_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m      \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_dir, train_batch_size, eval_batch_size, gradient_accumulation_steps, max_grad_norm, learning_rate, warmup_steps, num_steps, weight_decay, decay_type, device, eval_every)\u001b[0m\n\u001b[0;32m     51\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m Steps) (loss=\u001b[39m\u001b[38;5;132;01m%2.5f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (global_step, t_total, losses\u001b[38;5;241m.\u001b[39mval)\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m eval_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_acc \u001b[38;5;241m<\u001b[39m accuracy:\n\u001b[0;32m     57\u001b[0m         save_model(model, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMixer-B_16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 76\u001b[0m, in \u001b[0;36mvalid\u001b[1;34m(model, test_loader, global_step, device)\u001b[0m\n\u001b[0;32m     74\u001b[0m     all_label\u001b[38;5;241m.\u001b[39mappend(y\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     all_preds[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     77\u001b[0m         all_preds[\u001b[38;5;241m0\u001b[39m], preds\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     78\u001b[0m     )\n\u001b[0;32m     79\u001b[0m     all_label[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     80\u001b[0m         all_label[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     82\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating... (loss=\u001b[39m\u001b[38;5;132;01m%2.5f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m eval_losses\u001b[38;5;241m.\u001b[39mval)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "img_size = 224\n",
    "patch_size = 16\n",
    "hidden_dim = 128\n",
    "n_blocks = 4\n",
    "tokens_mlp_dim = 128\n",
    "channels_mlp_dim = 128\n",
    "n_classes=10\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Model & Tokenizer Setup\n",
    "model = MLPMixer(img_size=img_size, \n",
    "                 patch_size=patch_size, \n",
    "                 hidden_dim=hidden_dim,\n",
    "                 channels_mlp_dim=channels_mlp_dim, \n",
    "                 tokens_mlp_dim=tokens_mlp_dim, \n",
    "                 n_classes=n_classes, \n",
    "                 n_blocks=n_blocks)\n",
    "\n",
    "#model.load_from(np.load(args.pretrained_dir))\n",
    "model.to(device)\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Number of Parameters: {num_params} M\")\n",
    "\n",
    "\n",
    "# Training setup\n",
    "train_batch_size = 1 # Total batch size for training\n",
    "eval_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "max_grad_norm = 1.0 \n",
    "learning_rate = 3e-2\n",
    "warmup_steps = 500\n",
    "num_steps = 10000\n",
    "weight_decay = 0\n",
    "decay_type = \"cosine\"\n",
    "eval_every = 50\n",
    "\n",
    "# Training\n",
    "train(model, \n",
    "      data_dir=data_dir,\n",
    "      train_batch_size=train_batch_size, \n",
    "      eval_batch_size = eval_batch_size,\n",
    "      gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "      max_grad_norm=max_grad_norm, learning_rate=learning_rate, \n",
    "      warmup_steps=warmup_steps, \n",
    "      num_steps=num_steps, \n",
    "      weight_decay=weight_decay, \n",
    "      decay_type=decay_type, \n",
    "      eval_every=eval_every, \n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "569785bf-cef3-4335-86c0-b7c239b28b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa5e66-7569-4cb3-ab39-fd827adf4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    " parser.add_argument(\"--name\", required=True,\n",
    "                        help=\"Name of this run. Used for monitoring.\")\n",
    "    parser.add_argument(\"--model_type\", choices=[\"Mixer-B_16\", \"Mixer-L_16\",\n",
    "                                                 \"Mixer-B_16-21k\", \"Mixer-L_16-21k\"],\n",
    "                        default=\"Mixer-B_16\",\n",
    "                        help=\"Which model to use.\")\n",
    "    parser.add_argument(\"--pretrained_dir\", type=str, default=\"checkpoint/Mixer-B_16.npz\",\n",
    "                        help=\"Where to search for pretrained ViT models.\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"output\", type=str,\n",
    "                        help=\"The output directory where checkpoints will be written.\")\n",
    "\n",
    "    parser.add_argument(\"--train_batch_size\", default=512, type=int,\n",
    "                        help=\"Total batch size for training.\")\n",
    "    parser.add_argument(\"--eval_batch_size\", default=512, type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "    parser.add_argument(\"--eval_every\", default=100, type=int,\n",
    "                        help=\"Run prediction on validation set every so many steps.\"\n",
    "                             \"Will always run one evaluation at the end of training.\")\n",
    "\n",
    "    parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
    "                        help=\"The initial learning rate for SGD.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
    "                        help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--num_steps\", default=10000, type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
    "                        help=\"How to decay the learning rate.\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=500, type=int,\n",
    "                        help=\"Step of training to perform learning rate warmup for.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                        help=\"Max gradient norm.\")\n",
    "\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument('--fp16', action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "    parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
    "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "    parser.add_argument('--loss_scale', type=float, default=0,\n",
    "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                             \"Positive power of 2: static loss scaling value.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
